# from airflow import DAG
# from airflow.operators.bash_operator import BashOperator
# from datetime import datetime, timedelta
# import sys
# import os
# DATA_SOURCES_OUTDIR= "/mnt/c/Users/Public/doctolib-export"

# default_args = {
#     'owner': 'doctolib',
#     'start_date': datetime(2022, 1, 1),
#     'email_on_failure': False,
#     'email_on_retry': False,
#     'email_on_success': False,
#     'retries': 1,
#     'retry_delay': timedelta(minutes=5),
# }

# dag = DAG(
#     dag_id='source_data_dag',
#     default_args=default_args,
#     catchup=False,
#     schedule_interval=timedelta(days=1)
# )

# task1 = BashOperator(
#     task_id='check_for_files',
#     bash_command=""" set -e 
#                     if [ -f {{DATA_SOURCES_OUTDIR}}/appointments_by_centers.csv ] 
#                     && [ -f {{DATA_SOURCES_OUTDIR}}/stock.csv ] 
#                     && [ -f {{DATA_SOURCES_OUTDIR}}/vaccination_centers.csv ] 
#                     && [ -f {{DATA_SOURCES_OUTDIR}}/vaccination_vs_appointments.csv ]; 
#                         then
#                             echo "All files exist."
#                     else
#                         echo "One or more files do not exist."
#                         exit 1
#                     fi
# """,
#     dag=dag,
#     retries=2,
#     retry_delay=timedelta(minutes=5)
    
# )

# # task2 = BashOperator(
# #     task_id='task_name_2',
# #     bash_command='echo "Hello Again!"',
# #     dag=dag,
# # )

# task1 
